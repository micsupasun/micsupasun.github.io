---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<style>
a:link {
  text-decoration: none;
}

a:visited {
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

a:active {
  text-decoration: underline;
}
</style>

<!-- You can find my published articles on my [Google Scholar](google_scholar_website) profile. -->

<h2>Journals</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://link.springer.com/article/10.1007/s11263-024-02070-2"><strong><span class="gsc_a_at">Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal‑Viewpoint Alignment</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, J Liu, L Zheng, T Gedeon, P Koniusz</div>
				<div class="gs_gray">International Journal of Computer Vision (<strong>IJCV</strong>), 1-32</div>
			</td>
			<td class="gsc_a_c">(IJCV special issue on our ACCV'22)<br>[<font color="red"><strong>IF: 19.5</strong></font>] <br><a href="https://github.com/LeiWangR/JEANIE" style="color:#000000;"> Code </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/9895208"><strong><span class="gsc_a_at">Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition</span></strong></a>
				<div class="gs_gray">Z Qin, Y Liu, P Ji, D Kim, <strong>L Wang</strong>, B McKay, S Anwar, T Gedeon</div>
				<div class="gs_gray">IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 4783-4797</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 14.255</font></strong>]<br><a href="https://github.com/harutatsuakiyama/Angular-Skeleton-Encoding" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><strong data-bind="text: title"><a href="https://ieeexplore.ieee.org/document/9521829">Tensor Representations for Action Recognition</a></strong>
				<div class="gs_gray">P Koniusz,<strong> L Wang</strong>, A Cherian</div>
				<div class="gs_gray">IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>) 44 (2), 648-665</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 24.314</font></strong>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2020</span></td>
		</tr>
                <tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/8753686"><strong><span class="gsc_a_at">A Comparative Review of Recent Kinect-based Action Recognition Algorithms</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, DQ Huynh, P Koniusz</div>
				<div class="gs_gray">IEEE Transactions on Image Processing (<strong>TIP</strong>) 29 (1), 15-28</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">IF: 11.041</font></strong>]<br><a href="https://github.com/LeiWangR/HDG" style="color:#000000;">Dataset & Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2020</span></td>
		</tr>
        </tbody>
</table>  

<h2>Conferences</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2405.01461"><strong><span class="gsc_a_at">SATO: Stable Text-to-Motion Framework</span></strong></a>
				<div class="gs_gray">W Chen*, H Xiao*, E Zhang*, L Hu, <strong>L Wang</strong>, M Liu, C Chen</div>
				<div class="gs_gray">ACM Multimedia (<strong>ACM-MM</strong>) (* denotes equal contribution.)</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong>, accepted</font>]<br><a href="https://github.com/sato-team/Stable-Text-to-motion-Framework" style="color:#000000;">Code</a>, <a href="https://sato-team.github.io/Stable-Text-to-Motion-Framework/" style="color:#000000;">Project website</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2310.05615.pdf"><strong><span class="gsc_a_at">Adaptive Multi-head Contrastive Learning</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz, T Gedeon, L Zheng</div>
				<div class="gs_gray">European Conference on Computer Vision (<strong>ECCV</strong>)</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong>, accepted</font>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2402.03019.pdf"><strong><span class="gsc_a_at">Taylor Videos for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, X Yuan, T Gedeon, L Zheng</div>
				<div class="gs_gray">International Conference on Machine Learning (<strong>ICML</strong>)</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br><a href="https://github.com/LeiWangR/video-ar" style="color:#000000;">Code</a>, <a href="https://leiwangr.github.io/files/icml24-poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2110.05216.pdf"><strong><span class="gsc_a_at">High-order Tensor Pooling with Attention for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, K Sun, P Koniusz</div>
				<div class="gs_gray">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 3885-3889</div>
			</td>
			<td class="gsc_a_c">(An extension of our TPAMI&#39;20)<br>[<font color="red"><strong>B</strong>, oral</font>] <br><a href="https://github.com/LeiWangR/HoTP" style="color:#000000;"> Code (preprocessing)</a><br><a href="https://leiwangr.github.io/files/icassp24_hop_suppl.pdf" style="color:#000000;">Appendix</a>, <a href="https://leiwangr.github.io/files/icassp24_hot_lecture.pdf" style="color:#000000;">Slides</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF"> 
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2310.10059.pdf"><strong><span class="gsc_a_at">Flow Dynamics Correction for Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 3795-3799</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>B</strong></font>]<br> <a href="https://leiwangr.github.io/files/icassp24_hal_poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_3Mformer_Multi-Order_Multi-Mode_Transformer_for_Skeletal_Action_Recognition_CVPR_2023_paper.pdf"><strong><span class="gsc_a_at">3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 5620-5631</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong></font>]<br> <a href="https://leiwangr.github.io/files/cvpr23-poster.pdf" style="color:#000000;">Poster </a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Temporal-Viewpoint_Transportation_Plan_for_Skeletal_Few-shot_Action_Recognition_ACCV_2022_paper.pdf"><strong><span class="gsc_a_at">Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">Asian Conference on Computer Vision (ACCV), 307-326</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>B</strong>, oral, 4.9% acceptance rate, <br><strong>Best Student Paper Award</strong></font>]<br><a href="https://leiwangr.github.io/files/ACCV2022_Best Student Paper Award.pdf" style="color:#000000;">Award certificate</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810174.pdf"><strong><span class="gsc_a_at">Uncertainty-DTW for Time Series and Sequences</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">European Conference on Computer Vision (<strong>ECCV</strong>), 176-195</div>
			</td>
			<td class="gsc_a_c">[<font color="red"><strong>A*</strong>, oral, 2.7% acceptance rate</font>]<br><a href="https://github.com/LeiWangR/uDTW" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://dl.acm.org/doi/10.1145/3474085.3475572"><strong><span class="gsc_a_at">Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz</div>
				<div class="gs_gray">ACM International Conference on Multimedia (<strong>ACM-MM</strong>), 4324-4333</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">A*</font></strong>]<br><a href="https://github.com/LeiWangR/ODFSDF" style="color:#000000;">Code</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/9008573"><strong data-bind="text: title">Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs</strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, P Koniusz, DQ Huynh</div>
				<div class="gs_gray">IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 8698-8708</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">A*</font></strong>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="https://ieeexplore.ieee.org/document/8803051"><strong><span class="gsc_a_at">Loss Switching Fusion with Similarity Search for Video Classification</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, DQ Huynh, MR Mansour</div>
				<div class="gs_gray">26th IEEE International Conference on Image Processing (ICIP), 974-978</div>
			</td>
			<td class="gsc_a_c">[<strong><font color="red">B</font></strong><font color="red">, industrial research <br>+ 1 AU<strong>&nbsp;patent</strong></font>]<br><a href="https://github.com/LeiWangR/LSFNet" style="color:#000000;">Our dataset</a></td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
	</tbody>
</table>
<h2>Patents</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href=""><strong><span class="gsc_a_at">System and Method of Detecting Anomalies from Mass Data</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">US patent (provisional, SN 63/326,525)</div>
			</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2022</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="http://pericles.ipaustralia.gov.au/ols/auspat/applicationDetails.do?applicationNo=2019903775"><strong><span class="gsc_a_at">Method and System for Classifying Video Data</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, G Woods</div>
				<div class="gs_gray">AU Patent AU 2,019,903,775</div>
			</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="http://pericles.ipaustralia.gov.au/ols/auspat/applicationDetails.do?applicationNo=2019900316"><strong><span class="gsc_a_at">System and Method of Video Data Retrieval</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, M Reda Mansour, G Woods</div>
				<div class="gs_gray">AU Patent AU 2,019,900,316</div>
			</td>
<!-- 			<td class="gsc_a_c">&nbsp;</td> -->
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2019</span></td>
		</tr>
	</tbody>
</table>
<h2>Theses</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#FFE4E1">
			<td class="gsc_a_t"><a href="https://openresearch-repository.anu.edu.au/bitstream/1885/301236/1/ANU_PhD_Thesis_corrected.pdf"><strong><span class="gsc_a_at">Robust Human Action Modelling</span></strong></a><br />
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">PhD thesis<font color="blue">*</font>, The Australian National University</div>
			</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">Nov 2023</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFE4E1">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2112.08626.pdf"><strong><span class="gsc_a_at">Analysis and Evaluation of Kinect-based Action Recognition Algorithms</span></strong></a><br />
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">Master&rsquo;s thesis, The University of Western Australia</div>
			</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">Nov 2017</span></td>
		</tr>
	</tbody>
</table>
 
<h2>arXiv preprints</h2>
<table id="gsc_a_t">
	<tbody id="gsc_a_b">
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2407.03179"><strong><span class="gsc_a_at">Motion meets Attention: Video Motion Prompts</span></strong></a>
				<div class="gs_gray">Q Chen, <strong>L Wang</strong>, P Koniusz, T Gedeon</div>
				<div class="gs_gray">arXiv preprint arXiv:2407.03179</div>
			</td>
			<td class="gsc_a_c">Qixiang Chen conducted this research under the supervision of Lei Wang for his final year honors research project at ANU. He is a recipient of research sponsorship from Active Intelligence Australia Pty Ltd in Perth, Western Australia, including The Active Intelligence Research Challenge Award. [<a href="https://q1xiangchen.github.io/motion-prompts/" style="color:#000000;">Project website</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#FFFFE0">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2402.04857.pdf"><strong><span class="gsc_a_at">Advancing Video Anomaly Detection: A Concise Review and a New Dataset</span></strong></a>
				<div class="gs_gray">L Zhu, <strong>L Wang</strong>, A Raj, T Gedeon, C Chen</div>
				<div class="gs_gray">arXiv preprint arXiv:2402.04857</div>
			</td>
			<td class="gsc_a_c">Liyun Zhu conducted this research under the supervision of Lei Wang for master's final year research project at ANU. Liyun Zhu and Arjun Raj are recipients of research sponsorship from Active Intelligence Australia Pty Ltd in Perth, Western Australia, which includes The Active Intelligence Research Challenge Award. [<a href="https://msad-dataset.github.io" style="color:#000000;">Project website</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E5E4E2">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2405.10718"><strong><span class="gsc_a_at">SignLLM: Sign Languages Production Large Language Models</span></strong></a>
				<div class="gs_gray">S Fang, <strong>L Wang</strong>, C Zheng, Y Tian, C Chen</div>
				<div class="gs_gray">arXiv preprint arxiv:2405.10718</div>
			</td>
			<td class="gsc_a_c">Research report. [<a href="https://signllm.github.io" style="color:#000000;">Project website</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2404.13016.pdf"><strong><span class="gsc_a_at">Optimizing Calibration by Gaining Aware of Prediction Correctness</span></strong></a>
				<div class="gs_gray">Y Liu, <strong>L Wang</strong>, Y Zou, J Zou, L Zheng</div>
				<div class="gs_gray">arXiv preprint arXiv:2404.13016</div>
			</td>
			<td class="gsc_a_c">Research report. [<a href="https://github.com/liuyvchi/Correctness-aware-calibration" style="color:#000000;">Code</a>]</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2024</span></td>
		</tr>
		<tr class="gsc_a_tr" style="background-color:#E0FFFF">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2112.12668.pdf"><strong><span class="gsc_a_at">3D Skeleton-based Few-shot Action Recognition with JEANIE is not so Naïve</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong>, J Liu, P Koniusz</div>
				<div class="gs_gray">arXiv preprint arXiv:2112.12668</div>
			</td>
			<td class="gsc_a_c">An extended version has been accepted by ACCV&#39;22 [oral] and has been awarded the Sang Uk Lee Best Student Paper Award. The further extension of ACCV'22 has been accepted for publication by the IJCV special issue.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2021</span></td>
		</tr>
		<tr class="gsc_a_tr">
			<td class="gsc_a_t"><a href="https://arxiv.org/pdf/2309.15768.pdf"><strong><span class="gsc_a_at">AI in Software Engineering: Case Studies and Prospects</span></strong></a>
				<div class="gs_gray"><strong>L Wang</strong></div>
				<div class="gs_gray">arXiv preprint arXiv:2309.15768</div>
			</td>
			<td class="gsc_a_c">Technical Report. The author conducted this work while enrolled as a master's student at UWA, specifically for the CITS5502 Software Processes unit in 2017.</td>
			<td class="gsc_a_y"><span class="gsc_a_h gsc_a_hc gs_ibl">2017</span></td>
		</tr>
	</tbody>
</table>
<p>&nbsp;</p>

